<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Resume | My New Hugo Site</title>
<meta name="keywords" content="">
<meta name="description" content="YOGESH PATIL Atlanta, GA 30041 | patilyogesh@outlook.com | linkedIn
Principal ▪ Data Technology Leader/Engineer with more than 15 years of progressive experience and proven record to architect, design and develop data solutions of an organization including Data warehouse, Data Lakes, Data Domain APIs ▪ Deep understanding and knowledge of various next generation tools and technologies available in Big Data Domain and their application ▪ Innovator, passionate and dynamic team player ▪ Technology evangelist within organization to democratize data and tools that enable transformation to a data driven organization">
<meta name="author" content="">
<link rel="canonical" href="https://datatechyogi.github.io/my-blog/resume/">
<link crossorigin="anonymous" href="/my-blog/assets/css/stylesheet.3613efbd0b1772781e8f49935e973cae632a7f61471c05b17be155505ccf87b5.css" integrity="sha256-NhPvvQsXcngej0mTXpc8rmMqf2FHHAWxe&#43;FVUFzPh7U=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/my-blog/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://datatechyogi.github.io/my-blog/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://datatechyogi.github.io/my-blog/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://datatechyogi.github.io/my-blog/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://datatechyogi.github.io/my-blog/apple-touch-icon.png">
<link rel="mask-icon" href="https://datatechyogi.github.io/my-blog/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Resume" />
<meta property="og:description" content="YOGESH PATIL Atlanta, GA 30041 | patilyogesh@outlook.com | linkedIn
Principal ▪ Data Technology Leader/Engineer with more than 15 years of progressive experience and proven record to architect, design and develop data solutions of an organization including Data warehouse, Data Lakes, Data Domain APIs ▪ Deep understanding and knowledge of various next generation tools and technologies available in Big Data Domain and their application ▪ Innovator, passionate and dynamic team player ▪ Technology evangelist within organization to democratize data and tools that enable transformation to a data driven organization" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://datatechyogi.github.io/my-blog/resume/" /><meta property="article:section" content="" />
<meta property="article:published_time" content="2023-04-07T18:17:54-04:00" />
<meta property="article:modified_time" content="2023-04-07T18:17:54-04:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Resume"/>
<meta name="twitter:description" content="YOGESH PATIL Atlanta, GA 30041 | patilyogesh@outlook.com | linkedIn
Principal ▪ Data Technology Leader/Engineer with more than 15 years of progressive experience and proven record to architect, design and develop data solutions of an organization including Data warehouse, Data Lakes, Data Domain APIs ▪ Deep understanding and knowledge of various next generation tools and technologies available in Big Data Domain and their application ▪ Innovator, passionate and dynamic team player ▪ Technology evangelist within organization to democratize data and tools that enable transformation to a data driven organization"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Resume",
      "item": "https://datatechyogi.github.io/my-blog/resume/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Resume",
  "name": "Resume",
  "description": "YOGESH PATIL Atlanta, GA 30041 | patilyogesh@outlook.com | linkedIn\nPrincipal ▪ Data Technology Leader/Engineer with more than 15 years of progressive experience and proven record to architect, design and develop data solutions of an organization including Data warehouse, Data Lakes, Data Domain APIs ▪ Deep understanding and knowledge of various next generation tools and technologies available in Big Data Domain and their application ▪ Innovator, passionate and dynamic team player ▪ Technology evangelist within organization to democratize data and tools that enable transformation to a data driven organization",
  "keywords": [
    
  ],
  "articleBody": "YOGESH PATIL Atlanta, GA 30041 | patilyogesh@outlook.com | linkedIn\nPrincipal ▪ Data Technology Leader/Engineer with more than 15 years of progressive experience and proven record to architect, design and develop data solutions of an organization including Data warehouse, Data Lakes, Data Domain APIs ▪ Deep understanding and knowledge of various next generation tools and technologies available in Big Data Domain and their application ▪ Innovator, passionate and dynamic team player ▪ Technology evangelist within organization to democratize data and tools that enable transformation to a data driven organization\nTechnology Summary Big Data Tools: Hadoop, Hive, Apache HBase, Apache Spark,PySpark, Apache Kafka, Apache Sqoop, Flume, Apache NiFi, Google BigQuery, YARN\nDatabases: DataStax Cassandra No SQL DB, Oracle DB, PL/SQL, SQL, MySQL, Apache Drill\nBI Tools: Google Data Studio, Tableau, Re dash, Power BI, OBIEE\nETL Tools: Informatica PowerCenter and MDM, Data Stage, Sqoop\nLanguages: Python, R, R Studio, Scala, Core Java\nOther Tools: Docker, Solr, Elastic Search, Splunk, Linux Shell scripting, Git, Maven\nAmazon Web Services: S3, EMR, EKS, ECS, EC2, AWS Glue, DynamoDB\nCertification ▪ Amazon Web Services – Certified Solution Architect\n▪ AWS Certified Data Analytics – Specialty\n▪ Certified Apache Cassandra Developer\n▪ Snow flake certified solution architect\nProfessional Experience LPL Financials LLC – AVP Technology Bank Trust July 2019 – Till Date\nResponsible for successfully development and delivery of Bank Trust Core components in AWS cloud including data and APIs Accomplishments ▪ Successfully designed and developed scalable Data Ingestions pipelines using aws glue, pyspark and aws wrangler with minimal cost with no Production outages since May 2022. ▪ Extensively created, managed AWS deployments using CloudFormation, terraform and EKS infrastructure components e.g. namespace until Production. ▪ Trained 10+ LPL engineers within Multi Custody and Bank Trust on Various AWS Services. ▪ Initiated LPL Common Logging Library for python like .Net. ▪ Managed all aspects of Bank Trust core, one of the first few projects that follows all recently established architecture principles within LPL. ▪ Designed and lead development ETL/ELT pipelines using Apache Drill and Spark using Scala/PySpark.\nHashMap Inc, Sr. Big Data Developer, Jan 2019 to July 2019 Design, develop, deploy and test Big data Architecture using various open source tools and technologies. Evaluate emerging technologies for batch and real time ETL/ELT for performance tuning. Design cloud first data strategies using Snowflake, Databricks Spark and various cloud vendors computing products.\nAccomplishments ▪ Designed and Implemented complex data pipelines in Apache hive and NiFi to reduce data consumption using protobufs. ▪ Designed and developed data lake solutions using Hive and snowflake. ▪ Designed and Developed ETL/ELT pipelines using hql and PySpark and Scala with spark.\nVerizon Connect, Sr. Data Engineer, Jan 2013 to Dec 2018 Designed robust and idempotent Data and Analytics solutions including development and testing using Informatica, Spark big data framework, Cassandra and various other NoSQL DB, open source big data tools and technologies and furnished with insights, analytics, reports and recommendations enabling effective strategic planning across all business units using Tableau, Redash, and OBIEE.\nAccomplishments ▪ Redesigned strategic data pipelines based on Cloud first strategy increasing efficiency and performance by 50 percent. ▪ Developed Metadata, Master Data Management and Advanced Visualization using Tableau for Marketing Analytics. Created Data Models for Segmentation. ▪ Designed and developed innovative data pipelines between legacy system and home grown cloud and AWS platform using DataStax Cassandra, Apache Kafka, Apache Spark and Informatica. ▪ Enabled Ingesting high volume data from Telematics IoT platform for end to end analytical and reporting and cater to Operational/Business Intelligence. ▪ Enabled gathering and processing raw, structured, semi-structured and unstructured data using batch and real-time data processing ensuring data quality using Apache NiFi, Hadoop, Hive ▪ Enabled gathering and processing data using Apache Spark to be consumed by ML Models for predictive analytics to analyze patterns and predict DTC Codes before they occur. ▪ Identified Data gaps and Design and Develop Informatica workflow for ETL to fill these. ▪ Implemented CICD and automated build and deployment processes. ▪ Identified data silos and Democratized the data using notebook usage.\nNeumeric Technologies Corp Sr. BI/ETL Developer, Dec 2007 to Jan 2013 Responsible for Projects Execution, Architecture, Project compliance to BI Best Practices and Design Standards as part of BI COE Team Member. Designed Server Architecture and Enterprise reporting metadata to be consumed by various visualization tools e.g. OBIEE and Tableau desktop.\nAccomplishments ▪ Upgraded and implemented 6 BI Application for performance enhancements. ▪ Migrated 2 BI Implementation to Tableau to democratize data as part of data first approach. ▪ Designed logical tables, dimensions, dimensional Hierarchies, and logical keys, logical column aggregations such as % increase in assets under management using complex calculations, inbuilt functions and expressions as per business needs. ▪ Designed and developed, Modeled Data Marts using Star and snowflake schema and implemented ETLs using Informatica. ▪ Knowledgeable in cache purging strategies using Oracle BI event-polling table and other Purging Mechanisms. ▪ Sliced and diced data using filters and used pivot tables and charts for better presentation of most saleable product of the company. ▪ Created Enterprise dashboards using tableau desktop.\nOracle India Pvt. Ltd, Software Engineer, July 2006 to Dec 2007 As part of The PeopleSoft Fusion Analytics Product team developed and designed canned ETL in Data Stage and Informatica and people soft reports in OBIEE. This challenging project of PeopleSoft EPM supported worldwide clients and implementation partners of Oracle. Accomplishments:\n▪ Involved in the writing of the SQL queries to get the required the data from the database. ▪ Coded PL/SQL subroutines for data loading and scrubbing into the staging area. ▪ Performed SQL query optimization using hints and indexes. ▪ Created Data Models and Tuned Custom ETL jobs in Data Stage/Informatica for Customer Requirements.\nCognizant Technology Solutions, Programmer Analyst, Dec 2004 to July 2006 Designed and Developed Data Marts for WaMu using Informatica/Java and provided custom data packages to be used by Hyperion for reporting.\nAccomplishments ▪ Analyzed the business processes and needs with the clients and technical support agents.\n▪ Created Informatica mapping to populate the Staging Area. ▪ Created the Mapping Documents and UDDs for the Mapping created. ▪ Maintained the ETL code repository in VSS for audit purposes. ▪ Tuned the long running Informatica sessions. ▪ Involved in the writing of the SQL queries to get the required the data from the database.\nEducation RGPV – Indore (India) Bachelor of Engineering – Information Technology (2000 – 2004)\nVolunteerism ▪ Organizer – Atlanta Apache Cassandra Meetup\n▪ Volunteer – Heartfulness Institute.\n",
  "wordCount" : "1078",
  "inLanguage": "en",
  "datePublished": "2023-04-07T18:17:54-04:00",
  "dateModified": "2023-04-07T18:17:54-04:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://datatechyogi.github.io/my-blog/resume/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "My New Hugo Site",
    "logo": {
      "@type": "ImageObject",
      "url": "https://datatechyogi.github.io/my-blog/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://datatechyogi.github.io/my-blog/" accesskey="h" title="My New Hugo Site (Alt + H)">My New Hugo Site</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Resume
    </h1>
    <div class="post-meta"><span title='2023-04-07 18:17:54 -0400 -0400'>April 7, 2023</span>

</div>
  </header> 
  <div class="post-content"><h1 id="yogesh-patil"><strong>YOGESH PATIL</strong><a hidden class="anchor" aria-hidden="true" href="#yogesh-patil">#</a></h1>
<p>Atlanta, GA 30041 | <a href="mailto:patilyogesh@outlook.com">patilyogesh@outlook.com</a> | <a href="https://www.linkedin.com/in/yogeshpatilrk/">linkedIn</a></p>
<hr>
<h3 id="principal">Principal<a hidden class="anchor" aria-hidden="true" href="#principal">#</a></h3>
<hr>
<p>▪ Data Technology Leader/Engineer with more than 15 years of progressive experience and proven record to architect, design and develop data solutions of an organization including Data warehouse, Data Lakes, Data Domain APIs
▪ Deep understanding and knowledge of various next generation tools and technologies available in Big Data Domain and their application
▪ Innovator, passionate and dynamic team player
▪ Technology evangelist within organization to democratize data and tools that enable transformation to a data driven organization</p>
<h4 id="technology-summary">Technology Summary<a hidden class="anchor" aria-hidden="true" href="#technology-summary">#</a></h4>
<hr>
<p><strong>Big Data Tools:</strong>  Hadoop, Hive, Apache HBase, Apache Spark,PySpark, Apache Kafka, Apache Sqoop, Flume, Apache NiFi, Google BigQuery, YARN<br>
Databases: DataStax Cassandra No SQL DB, Oracle DB, PL/SQL, SQL, MySQL, Apache Drill<br>
<strong>BI Tools</strong>: Google Data Studio, Tableau, Re dash, Power BI, OBIEE<br>
<strong>ETL Tools:</strong> Informatica PowerCenter and MDM, Data Stage, Sqoop<br>
<strong>Languages:</strong> Python, R, R Studio, Scala, Core Java<br>
<strong>Other Tools</strong>: Docker, Solr, Elastic Search, Splunk, Linux Shell scripting, Git, Maven<br>
<strong>Amazon Web Services:</strong> S3, EMR, EKS, ECS, EC2, AWS Glue, DynamoDB</p>
<h4 id="certification">Certification<a hidden class="anchor" aria-hidden="true" href="#certification">#</a></h4>
<hr>
<p>▪ Amazon Web Services – Certified Solution Architect<br>
▪ AWS Certified Data Analytics – Specialty<br>
▪ Certified Apache Cassandra Developer<br>
▪ Snow flake certified solution architect</p>
<h4 id="professional-experience">Professional Experience<a hidden class="anchor" aria-hidden="true" href="#professional-experience">#</a></h4>
<hr>
<p>LPL Financials LLC – AVP Technology Bank Trust July 2019 – Till Date</p>
<p>Responsible for successfully development and delivery of Bank Trust Core components in AWS cloud including data and APIs
Accomplishments
▪ Successfully designed and developed scalable Data Ingestions pipelines using aws glue, pyspark and aws wrangler with minimal cost with no Production outages since May 2022.
▪ Extensively created, managed AWS deployments using CloudFormation, terraform and EKS infrastructure components e.g. namespace until Production.
▪ Trained 10+ LPL engineers within Multi Custody and Bank Trust on Various AWS Services.
▪ Initiated LPL Common Logging Library for python like .Net.
▪ Managed all aspects of Bank Trust core, one of the first few projects that follows all recently established architecture principles within LPL.
▪ Designed and lead development ETL/ELT pipelines using Apache Drill and Spark using Scala/PySpark.</p>
<p>HashMap Inc, Sr. Big Data Developer, Jan 2019 to July 2019
Design, develop, deploy and test Big data Architecture using various open source tools and technologies.
Evaluate emerging technologies for batch and real time ETL/ELT for performance tuning. Design cloud first data strategies using Snowflake, Databricks Spark and various cloud vendors computing products.</p>
<h4 id="accomplishments">Accomplishments<a hidden class="anchor" aria-hidden="true" href="#accomplishments">#</a></h4>
<hr>
<p>▪ Designed and Implemented complex data pipelines in Apache hive and NiFi to reduce data consumption using protobufs.
▪ Designed and developed data lake solutions using Hive and snowflake.
▪ Designed and Developed ETL/ELT pipelines using hql and PySpark and Scala with spark.</p>
<p>Verizon Connect, Sr. Data Engineer, Jan 2013 to Dec 2018
Designed robust and idempotent Data and Analytics solutions including development and testing using Informatica, Spark big data framework, Cassandra and various other NoSQL DB, open source big data tools and technologies and furnished with insights, analytics, reports and recommendations enabling effective strategic planning across all business units using Tableau, Redash, and OBIEE.</p>
<h4 id="accomplishments-1">Accomplishments<a hidden class="anchor" aria-hidden="true" href="#accomplishments-1">#</a></h4>
<hr>
<p>▪ Redesigned strategic data pipelines based on Cloud first strategy increasing efficiency and performance by 50 percent.
▪ Developed Metadata, Master Data Management and Advanced Visualization using Tableau for Marketing Analytics. Created Data Models for Segmentation.
▪ Designed and developed innovative data pipelines between legacy system and home grown cloud and AWS platform using DataStax Cassandra, Apache Kafka, Apache Spark and Informatica.
▪ Enabled Ingesting high volume data from Telematics IoT platform for end to end analytical and reporting and cater to Operational/Business Intelligence.
▪ Enabled gathering and processing raw, structured, semi-structured and unstructured data using batch and real-time data processing ensuring data quality using Apache NiFi, Hadoop, Hive
▪ Enabled gathering and processing data using Apache Spark to be consumed by ML Models for predictive analytics to analyze patterns and predict DTC Codes before they occur.
▪ Identified Data gaps and Design and Develop Informatica workflow for ETL to fill these.
▪ Implemented CICD and automated build and deployment processes.
▪ Identified data silos and Democratized the data using notebook usage.</p>
<p>Neumeric Technologies Corp Sr. BI/ETL Developer, Dec 2007 to Jan 2013
Responsible for Projects Execution, Architecture, Project compliance to BI Best Practices and Design Standards as part of BI COE Team Member. Designed Server Architecture and Enterprise reporting metadata to be consumed by various visualization tools e.g. OBIEE and Tableau desktop.</p>
<h4 id="accomplishments-2">Accomplishments<a hidden class="anchor" aria-hidden="true" href="#accomplishments-2">#</a></h4>
<hr>
<p>▪ Upgraded and implemented 6 BI Application for performance enhancements.
▪ Migrated 2 BI Implementation to Tableau to democratize data as part of data first approach.
▪ Designed logical tables, dimensions, dimensional Hierarchies, and logical keys, logical column aggregations such as % increase in assets under management using complex calculations, inbuilt functions and expressions as per business needs.
▪ Designed and developed, Modeled Data Marts using Star and snowflake schema and implemented ETLs using Informatica.
▪ Knowledgeable in cache purging strategies using Oracle BI event-polling table and other Purging Mechanisms.
▪ Sliced and diced data using filters and used pivot tables and charts for better presentation of most saleable product of the company.
▪ Created Enterprise dashboards using tableau desktop.</p>
<p>Oracle India Pvt. Ltd, Software Engineer, July 2006 to Dec 2007
As part of The PeopleSoft Fusion Analytics Product team developed and designed canned ETL in Data Stage and Informatica and people soft reports in OBIEE. This challenging project of PeopleSoft EPM supported worldwide clients and implementation partners of Oracle.
Accomplishments:</p>
<p>▪ Involved in the writing of the SQL queries to get the required the data from the database.
▪ Coded PL/SQL subroutines for data loading and scrubbing into the staging area.
▪ Performed SQL query optimization using hints and indexes.
▪ Created Data Models and Tuned Custom ETL jobs in Data Stage/Informatica for Customer Requirements.</p>
<p>Cognizant Technology Solutions, Programmer Analyst, Dec 2004 to July 2006
Designed and Developed Data Marts for WaMu using Informatica/Java and provided custom data packages to be used by Hyperion for reporting.</p>
<h4 id="accomplishments-3">Accomplishments<a hidden class="anchor" aria-hidden="true" href="#accomplishments-3">#</a></h4>
<hr>
<p>▪ Analyzed the business processes and needs with the clients and technical support agents.<br>
▪ Created Informatica mapping to populate the Staging Area.
▪ Created the Mapping Documents and UDDs for the Mapping created.
▪ Maintained the ETL code repository in VSS for audit purposes.
▪ Tuned the long running Informatica sessions.
▪ Involved in the writing of the SQL queries to get the required the data from the database.</p>
<h4 id="education">Education<a hidden class="anchor" aria-hidden="true" href="#education">#</a></h4>
<hr>
<p>RGPV – Indore (India)
Bachelor of Engineering – Information Technology (2000 – 2004)</p>
<h4 id="volunteerism">Volunteerism<a hidden class="anchor" aria-hidden="true" href="#volunteerism">#</a></h4>
<hr>
<p>▪ <strong>Organizer</strong> – Atlanta Apache Cassandra Meetup<br>
▪ <strong>Volunteer</strong> – Heartfulness Institute.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://datatechyogi.github.io/my-blog/">My New Hugo Site</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
